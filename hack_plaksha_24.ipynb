!pip install pymongo "pymongo[srv]" langchain transformers accelerate
# !pip install cchardet
import os
import cchardet
from pymongo.mongo_client import MongoClient
from pymongo.server_api import ServerApi
import re
from langchain.chains import LLMChain, ConstitutionalChain
from langchain.chains.constitutional_ai.models import ConstitutionalPrinciple
from langchain_core.prompts.prompt import PromptTemplate
from langchain import HuggingFaceHub
from transformers import T5ForConditionalGeneration, T5Tokenizer
import torch
import accelerate

from google.colab import userdata
uri = f"mongodb+srv://sridharananish5:{userdata.get('mongo_pw')}@cluster0.t8s7xp5.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0"
#mongo_pw hidden for security purposes.

# Create a new client and connect to the server
client = MongoClient(uri, server_api=ServerApi('1'))
db = client["un_database"] #the database name
all_speeches = db["all_speeches"] #the table name

# Path to the "master" folder with the full corpus in .txt files.
master_folder_path = "/content/drive/MyDrive/TXT"

# Use the code below to repopulate the MongoDB database.

'''

all_speeches.drop()
#every runtime the columns get added and may be repeated, so we are deleting and starting anew.

all_speeches = db["all_speeches"] #creating anew

# The "master" folder has multiple folders for each session.
for session_folder in os.listdir(master_folder_path):
  session_folder_path = os.path.join(master_folder_path, session_folder)

  if os.path.isdir(session_folder_path):
    # Each folder name is of the form "Session xx - yyyy" where xx is the session number and yyyy is the year.
    session_info = session_folder.split("-")
    session_number = session_info[0].strip().split()[-1]
    year = session_info[1]

    # Each session folder has multiple .txt files for each speech.
    for text_file in os.listdir(session_folder_path):
      if text_file.endswith(".txt"):

        #some corrupted files with macOS metadata that always start with "._" showed up in the corpus. skipping.
        if text_file.startswith("._"):
          continue

        # each file name is of the form "ccc_xx_yyyy.txt" where ccc is the country code.
        country_code = text_file.split("_")[0]
        text_file_path = os.path.join(session_folder_path, text_file)

        # Not every text file has the same encoding. Using chardet.
        with open(text_file_path,"rb") as f:
          result=cchardet.detect(f.read())

        with open(text_file_path,"r",encoding=result["encoding"]) as f:
          file_content = f.read()

        # Insert into our table.
        all_speeches.insert_one({
            "year": year,
            "session_number": session_number,
            "country_code": country_code,
            "raw_text": file_content
            })
'''
# use this code to preprocess the dataset for use to train llm.

'''
# database ready! time to train llm on it.
# columns- year of speech, UN session number, country code for the speaker, raw text of the speech.

speeches = all_speeches.find()
training_data = []

for speech in speeches:
    year = speech['year']
    session_no = speech['session_number']
    country = speech['country_code']
    text = speech['raw_text']

    # Preprocess the text- remove spl chars, lowercase
    processed_text = re.sub(r'\W+', ' ', text)  # Remove non-alphanumeric characters
    processed_text = processed_text.lower()

    # Create a training example
    training_example = f"this speech was made by the representative of the country with country code {country.lower()} in the year {year} during the un general debate s session number {session_no} {processed_text}"
    training_data.append(training_example)

# training data ready!


# dataset tokenising
tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-small")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-small")
tokenized_dataset = tokenizer(training_data, return_tensors="pt", padding=True, truncation=True)
# create PyTorch dataset
input_ids = tokenized_dataset["input_ids"]
attention_masks = tokenized_dataset["attention_mask"]
dataset = torch.utils.data.TensorDataset(input_ids, attention_masks)
torch.save(dataset, "preprocessed_dataset.pt")

'''

train_dataset = torch.load("/content/preprocessed_dataset.pt")

qa_prompt = PromptTemplate(
    template="Q: {question} A:",
    input_variables=["question"],
)

llm=HuggingFaceHub(repo_id="google/flan-t5-small",huggingfacehub_api_token="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", model_kwargs={"temperature":0.1})

qa_chain = LLMChain(llm=llm, prompt=qa_prompt)

constitutional_principles = [
    ConstitutionalPrinciple(
        critique_request="Does this answer cite the sources where it got it's data from?",
        revision_request="Cite all relevant sources in your answer.",
    ),
    ConstitutionalPrinciple(
        critique_request="Does the answer get any data from outside the UN General Debate speeches?",
        revision_request="Answer using only the data of UN General Debate speeches you have been trained on.",
    ),
    ConstitutionalPrinciple(
        critique_request="Are all the facts in this answer accurate as per your knowledge?",
        revision_request="Ensure your entire answer is accurate as per your training data. If you cannot make sure of it, add to the answer- I may not be able to guarantee the accuracy of this answer.",
    )
]

constitutional_chain = ConstitutionalChain.from_llm(
    llm=llm,
    chain=qa_chain,
    constitutional_principles=constitutional_principles,
)

question=""

while question!="end":
  question=input("Please ask a question, or type end to stop.")
  if question=="end":
    print("Ok, thank you!")
  else:
    constitutional_chain.invoke({"question":question})
